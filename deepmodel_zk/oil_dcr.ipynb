{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepctr_torch.models import DeepFM\n",
    "from deepctr_torch.inputs import  SparseFeat, DenseFeat,get_feature_names\n",
    "import torch\n",
    "data = pd.read_csv('./criteo_sample.txt')\n",
    "\n",
    "sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "dense_features = ['I'+str(i) for i in range(1, 14)]\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0,)\n",
    "target = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0,1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                        for feat in sparse_features]\n",
    "dense_feature_columns = [DenseFeat(feat, 1)\n",
    "                      for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, dimension=1e6,use_hash=True) for feat in sparse_features]#The dimension can be set according to data\n",
    "dense_feature_columns = [DenseFeat(feat, 1)\n",
    "                      for feat in dense_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "linear_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name] for name in feature_names}\n",
    "\n",
    "test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "model = DeepFM(linear_feature_columns,dnn_feature_columns,task='binary',device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=['binary_crossentropy'], )\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values,\n",
    "                    batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n",
      "Epoch 1/10\n",
      "1s - loss:  0.6975 - binary_crossentropy:  0.6975 - auc:  0.4355 - val_binary_crossentropy:  0.6656 - val_auc:  0.4286\n",
      "Epoch 2/10\n",
      "0s - loss:  0.6088 - binary_crossentropy:  0.6088 - auc:  0.9538 - val_binary_crossentropy:  0.6477 - val_auc:  0.5801\n",
      "Epoch 3/10\n",
      "0s - loss:  0.4963 - binary_crossentropy:  0.4963 - auc:  0.9993 - val_binary_crossentropy:  0.6503 - val_auc:  0.5498\n",
      "Epoch 4/10\n",
      "0s - loss:  0.4042 - binary_crossentropy:  0.4042 - auc:  0.9993 - val_binary_crossentropy:  0.6705 - val_auc:  0.5238\n",
      "Epoch 5/10\n",
      "0s - loss:  0.3120 - binary_crossentropy:  0.3120 - auc:  0.9997 - val_binary_crossentropy:  0.7140 - val_auc:  0.5195\n",
      "Epoch 6/10\n",
      "0s - loss:  0.2245 - binary_crossentropy:  0.2245 - auc:  1.0000 - val_binary_crossentropy:  0.7790 - val_auc:  0.5411\n",
      "Epoch 7/10\n",
      "0s - loss:  0.1524 - binary_crossentropy:  0.1524 - auc:  1.0000 - val_binary_crossentropy:  0.8525 - val_auc:  0.5455\n",
      "Epoch 8/10\n",
      "0s - loss:  0.1017 - binary_crossentropy:  0.1017 - auc:  1.0000 - val_binary_crossentropy:  0.9324 - val_auc:  0.5325\n",
      "Epoch 9/10\n",
      "0s - loss:  0.0692 - binary_crossentropy:  0.0692 - auc:  1.0000 - val_binary_crossentropy:  0.9876 - val_auc:  0.5325\n",
      "Epoch 10/10\n",
      "0s - loss:  0.0489 - binary_crossentropy:  0.0489 - auc:  1.0000 - val_binary_crossentropy:  1.0752 - val_auc:  0.5368\n",
      "\n",
      "test LogLoss 0.4879\n",
      "test AUC 0.6667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models import *\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('./criteo_sample.txt')\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train_model_input = {name:train[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "\n",
    "    device = 'cpu'\n",
    "    use_cuda = False\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "    model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "    model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "                  metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "    model.fit(train_model_input, train[target].values,\n",
    "              batch_size=256, epochs=10, validation_split=0.2, verbose=2)\n",
    "\n",
    "    pred_ans = model.predict(test_model_input, 256)\n",
    "    print(\"\")\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([      0,       1,       2,       3,       4,       5,       6,       7,\n",
       "             8,       9,      10,      11,      12,      13,      14,      15,\n",
       "            16,      17,      18,      19,      20,      21,      22,      23,\n",
       "            24,      25,      26,      27,      28,      29,      30,      31,\n",
       "            32,      33,      34,      35,      36,      37,      38,      39,\n",
       "            40,      41,      42,      43,      44,      45,      46,      47,\n",
       "            48,      49,      50,      51,      52,      53,      54,      55,\n",
       "            56,      57,      58,      59,      60,      61,      62,      63,\n",
       "            64,      65,      66,      67,      68,      69,      70,      71,\n",
       "            72,      73,      74,      75, 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(76))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./df_train_76d_full.pkl')\n",
    "dense_features =[0,       1,       2,       3,       4,       5,       6,       7,\n",
    "             8,       9,      10,      11,      12,      13,      14,      15,\n",
    "            16,      17,      18,      19,      20,      21,      22,      23,\n",
    "            24,      25,      26,      27,      28,      29,      30,      31,\n",
    "            32,      33,      34,      35,      36,      37,      38,      39,\n",
    "            40,      41,      42,      43,      44,      45,      46,      47,\n",
    "            48,      49,      50,      51,      52,      53,      54,      55,\n",
    "            56,      57,      58,      59,      60,      61,      62,      63,\n",
    "            64,      65,      66,      67,      68,      69,      70,      71,\n",
    "            72,      73,      74,      75,]\n",
    "data[dense_features] = data[dense_features].fillna(0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('./df_train_76d_full.pkl')\n",
    "data_test = pd.read_pickle('./df_test_76d_full.pkl')\n",
    "data_val = pd.read_pickle('./df_val_76d_full.pkl')\n",
    "data=pd.concat([data_train, data_test,data_val], axis=0, ignore_index=True)\n",
    "data.to_pickle(\"./df_76d_full_50942.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models import *\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_pickle('./df_76d_full_50942.pkl')\n",
    "    sparse_features = []\n",
    "    dense_features =[0,       1,       2,       3,       4,       5,       6,       7,\n",
    "             8,       9,      10,      11,      12,      13,      14,      15,\n",
    "            16,      17,      18,      19,      20,      21,      22,      23,\n",
    "            24,      25,      26,      27,      28,      29,      30,      31,\n",
    "            32,      33,      34,      35,      36,      37,      38,      39,\n",
    "            40,      41,      42,      43,      44,      45,      46,      47,\n",
    "            48,      49,      50,      51,      52,      53,      54,      55,\n",
    "            56,      57,      58,      59,      60,      61,      62,      63,\n",
    "            64,      65,      66,      67,      68,      69,      70,      71,\n",
    "            72,      73,      74,      75,]\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train_model_input = {name:train[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "\n",
    "    device = 'cpu'\n",
    "    use_cuda = False\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "    model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "    model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "                  metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "    model.fit(train_model_input, train[target].values,\n",
    "              batch_size=256, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "    pred_ans = model.predict(test_model_input, 256)\n",
    "    print(\"\")\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 32602 samples, validate on 8151 samples, 128 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:05,  7.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2a9a36f9c5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m                   metrics=[\"binary_crossentropy\", \"auc\"],)\n\u001b[1;32m     64\u001b[0m     model.fit(train_model_input, train[target].values,\n\u001b[0;32m---> 65\u001b[0;31m               batch_size=256, epochs=100, validation_split=0.2, verbose=1)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mpred_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle)\u001b[0m\n\u001b[1;32m    224\u001b[0m                         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                         \u001b[0mtotal_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sw/Anaconda2/envs/aliatte/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.models import *\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "import torch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_pickle('./df_76d_full_50942.pkl')\n",
    "    sparse_features = []\n",
    "    dense_features =[0,       1,       2,       3,       4,       5,       6,       7,\n",
    "             8,       9,      10,      11,      12,      13,      14,      15,\n",
    "            16,      17,      18,      19,      20,      21,      22,      23,\n",
    "            24,      25,      26,      27,      28,      29,      30,      31,\n",
    "            32,      33,      34,      35,      36,      37,      38,      39,\n",
    "            40,      41,      42,      43,      44,      45,      46,      47,\n",
    "            48,      49,      50,      51,      52,      53,      54,      55,\n",
    "            56,      57,      58,      59,      60,      61,      62,      63,\n",
    "            64,      65,      66,      67,      68,      69,      70,      71,\n",
    "            72,      73,      74,      75,]\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                              for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                              for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train_model_input = {name:train[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "\n",
    "    device = 'cpu'\n",
    "    use_cuda = False\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "    model = DCN(dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "                   l2_reg_embedding=1e-5, device=device)\n",
    "\n",
    "    model.compile(\"adagrad\", \"binary_crossentropy\",\n",
    "                  metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "    model.fit(train_model_input, train[target].values,\n",
    "              batch_size=256, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "    pred_ans = model.predict(test_model_input, 256)\n",
    "    print(\"\")\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n",
      "Epoch 1/10\n",
      "0s - loss:  0.6996 - binary_crossentropy:  0.6996 - val_binary_crossentropy:  0.6909\n",
      "Epoch 2/10\n",
      "0s - loss:  0.6857 - binary_crossentropy:  0.6857 - val_binary_crossentropy:  0.6812\n",
      "Epoch 3/10\n",
      "0s - loss:  0.6712 - binary_crossentropy:  0.6712 - val_binary_crossentropy:  0.6722\n",
      "Epoch 4/10\n",
      "0s - loss:  0.6574 - binary_crossentropy:  0.6574 - val_binary_crossentropy:  0.6635\n",
      "Epoch 5/10\n",
      "0s - loss:  0.6438 - binary_crossentropy:  0.6438 - val_binary_crossentropy:  0.6550\n",
      "Epoch 6/10\n",
      "0s - loss:  0.6301 - binary_crossentropy:  0.6301 - val_binary_crossentropy:  0.6467\n",
      "Epoch 7/10\n",
      "0s - loss:  0.6165 - binary_crossentropy:  0.6165 - val_binary_crossentropy:  0.6388\n",
      "Epoch 8/10\n",
      "0s - loss:  0.6033 - binary_crossentropy:  0.6033 - val_binary_crossentropy:  0.6314\n",
      "Epoch 9/10\n",
      "0s - loss:  0.5904 - binary_crossentropy:  0.5904 - val_binary_crossentropy:  0.6243\n",
      "Epoch 10/10\n",
      "0s - loss:  0.5777 - binary_crossentropy:  0.5777 - val_binary_crossentropy:  0.6178\n",
      "test LogLoss 0.6398\n",
      "test AUC 0.6131\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from deepctr_torch.models import DIN\n",
    "from deepctr_torch.inputs import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
    "\n",
    "\n",
    "def get_xy_fd():\n",
    "\n",
    "    feature_columns = [SparseFeat('user',3),SparseFeat(\n",
    "        'gender', 2), SparseFeat('item', 3 + 1), SparseFeat('item_gender', 2 + 1),DenseFeat('score', 1)]\n",
    "    feature_columns += [VarLenSparseFeat('hist_item',3+1, maxlen=4, embedding_name='item'),\n",
    "                        VarLenSparseFeat('hist_item_gender',3+1, maxlen=4, embedding_name='item_gender')]\n",
    "\n",
    "    behavior_feature_list = [\"item\", \"item_gender\"]\n",
    "    uid = np.array([0, 1, 2])\n",
    "    ugender = np.array([0, 1, 0])\n",
    "    iid = np.array([1, 2, 3])  # 0 is mask value\n",
    "    igender = np.array([1, 2, 1])  # 0 is mask value\n",
    "    score = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "    hist_iid = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]])\n",
    "    hist_igender = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [2, 1, 0, 0]])\n",
    "\n",
    "    feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'item_gender': igender,\n",
    "                    'hist_item': hist_iid, 'hist_item_gender': hist_igender, 'score': score}\n",
    "    x = {name:feature_dict[name] for name in get_feature_names(feature_columns)}\n",
    "    y = [1, 0, 1]\n",
    "    return x, y, feature_columns, behavior_feature_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x, y, feature_columns, behavior_feature_list = get_xy_fd()\n",
    "    model = DIN(feature_columns, behavior_feature_list, hist_len_max=4, )\n",
    "    model.compile('adam', 'binary_crossentropy',\n",
    "                  metrics=['binary_crossentropy'])\n",
    "    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aliatte]",
   "language": "python",
   "name": "conda-env-aliatte-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
